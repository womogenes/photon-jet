{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "59faa34e-adca-4477-905a-0e06e1d5e9d6",
   "metadata": {},
   "source": [
    "# Scalar preprocessing with sparse images\n",
    "\n",
    "A lot of the images don't have particles everywhere. Let's try excluding the cells where energy is zero."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "494687ac-160c-432c-8f23-56eda0a4984e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Importing computational stuff...\n",
      "Importing display stuff...\n",
      "Importing utilities...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-06-24 11:03:36.565015: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-06-24 11:04:39.347990: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "print(f\"Importing computational stuff...\")\n",
    "import sys; sys.path.append(\"..\")\n",
    "\n",
    "import os\n",
    "import time\n",
    "import h5py\n",
    "import numpy as np\n",
    "import math\n",
    "import yaml\n",
    "\n",
    "print(f\"Importing display stuff...\")\n",
    "from tqdm import tqdm, trange\n",
    "from pprint import pprint\n",
    "\n",
    "print(f\"Importing utilities...\")\n",
    "from utils import convert_size, data_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "491bdf73-204f-4e90-8f9f-22be7425ac76",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading datasets... (~5 sec)\n"
     ]
    }
   ],
   "source": [
    "# ~5 sec\n",
    "print(f\"Loading datasets... (~5 sec)\")\n",
    "\n",
    "os.chdir(f\"{data_dir}/npz\")\n",
    "raw_pions = dict(np.load(\"pi0_40-250GeV_100k.npz\"))\n",
    "raw_photons = dict(np.load(\"gamma_40-250GeV_100k.npz\"))\n",
    "raw_scalars = dict(np.load(\"scalar1_40-250GeV_100k.npz\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "3e53480b-a2a0-41bd-a6cd-af3b3619f32d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def norm_coords(n):\n",
    "    \"\"\"\n",
    "    Generate list of n consecutive numbers, normally distributed.\n",
    "        e.g. norm_coords(4) -> [-1.34, -0.45, 0.45, 1.34]\n",
    "    \"\"\"\n",
    "    x = np.arange(n)\n",
    "    return (x - np.mean(x)) / np.std(x)\n",
    "\n",
    "def to_cloud(arr, tag, treshold=1e-5):\n",
    "    \"\"\"\n",
    "    Turn arr of shape (samples, rows, cols) into point clouds.\n",
    "    Point cloud looks like (samples, points, features).\n",
    "    Features will be a 4-vector of (eta, phi, energy, tag).\n",
    "        tag will probably be layer #\n",
    "    Points with energy < threshold will be zeroed out.\n",
    "    \n",
    "    Points may be ragged; they will be padded in that case.\n",
    "    \"\"\"\n",
    "    n_samples, n_rows, n_cols = arr.shape\n",
    "    img_shape = (n_rows, n_cols)\n",
    "    n_points = n_rows * n_cols\n",
    "    \n",
    "    # This shape rebroadcast can take a bit to wrap your head around\n",
    "    row_coords = np.broadcast_to(norm_coords(n_rows)[:, None], img_shape)\n",
    "    col_coords = np.broadcast_to(norm_coords(n_cols)[None, :], img_shape)\n",
    "    \n",
    "    coords = np.stack((row_coords, col_coords), axis=2).reshape((n_points, -1))\n",
    "    coords = np.expand_dims(coords, axis=0)\n",
    "    \n",
    "    coords = np.broadcast_to(coords, (n_samples, n_points, 2))\n",
    "    new_arr = np.expand_dims(np.reshape(arr, (n_samples, -1)), axis=2)\n",
    "    tag_arr = np.broadcast_to([[[tag]]], (n_samples, n_points, 1))\n",
    "    \n",
    "    full_cloud = np.concatenate((coords, new_arr, tag_arr), axis=2)\n",
    "    \n",
    "    return full_cloud * (full_cloud[:,:,2] > 1e-5)[:,:,np.newaxis]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "2d085f20-173b-4ed5-b88d-4164412177c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing all datasets...\n",
      "Processing pions...\n",
      "    Processing layer_0...\n",
      "    Processing layer_1...\n",
      "    Processing layer_2...\n",
      "    Processing layer_3...\n",
      "Processing photons...\n",
      "    Processing layer_0...\n",
      "    Processing layer_1...\n",
      "    Processing layer_2...\n",
      "    Processing layer_3...\n",
      "Processing scalars...\n",
      "    Processing layer_0...\n",
      "    Processing layer_1...\n",
      "    Processing layer_2...\n",
      "    Processing layer_3...\n"
     ]
    }
   ],
   "source": [
    "# Process all datasets\n",
    "def process_dataset(dataset):\n",
    "    res = []\n",
    "    layers = [f\"layer_{i}\" for i in range(4)]\n",
    "    \n",
    "    for i, layer in enumerate(layers):\n",
    "        print(f\"    Processing {layer}...\")\n",
    "        res.append(to_cloud(dataset[layer], tag=i))\n",
    "        \n",
    "    return np.concatenate(res, axis=1)\n",
    "\n",
    "print(f\"Processing all datasets...\")\n",
    "raw_datasets = {\"pions\": raw_pions, \"photons\": raw_photons, \"scalars\": raw_scalars}\n",
    "processed = {}\n",
    "\n",
    "for class_type, dataset in raw_datasets.items():\n",
    "    print(f\"Processing {class_type}...\")\n",
    "    processed[class_type] = process_dataset(raw_datasets[class_type])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "ef79553d-f8ba-48ab-aa75-bb2e627e12af",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mixing up all jets... (~5 sec)\n",
      "All jets take up 8.58 GB\n"
     ]
    }
   ],
   "source": [
    "# Mixing up all the jets\n",
    "print(f\"Mixing up all jets... (~5 sec)\")\n",
    "N = 100000\n",
    "all_jets = np.concatenate(list(processed.values()), axis=0)\n",
    "labels = np.array((0,) * N + (1,) * N + (2,) * N)\n",
    "\n",
    "assert(len(labels) == len(all_jets))\n",
    "order = np.random.permutation(len(labels))\n",
    "\n",
    "all_jets = all_jets[order]\n",
    "labels = labels[order]\n",
    "print(f\"All jets take up {convert_size(all_jets.nbytes)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "a9e14d89-3228-4b59-abf8-e867a4ab0bf5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving jets... (~10 sec)\n"
     ]
    }
   ],
   "source": [
    "# ~10 sec\n",
    "print(f\"Saving jets... (~10 sec)\")\n",
    "os.makedirs(f\"{data_dir}/processed/scalar\", exist_ok=True)\n",
    "os.chdir(f\"{data_dir}/processed/scalar\")\n",
    "np.savez(f\"all_jets_sparse_point_cloud.npz\", X=all_jets, y=labels)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "top-tagging",
   "language": "python",
   "name": "top-tagging"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
