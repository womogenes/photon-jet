{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c8e5aa65-83fc-42de-b61b-341213f701f1",
   "metadata": {},
   "source": [
    "# Photon jet data organizer\n",
    "\n",
    "Paper draft: https://arxiv.org/abs/2203.16703"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f9b143ac-d137-419f-bb23-6b833889941e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import h5py\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "from pprint import pprint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b8dc9d19-a86c-4481-967d-fb1d30277dc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Constants and other data things\n",
    "data_dir = \"/usatlas/atlas01/atlasdisk/users/atlas_wifeng/photon-jet/data\"\n",
    "os.chdir(data_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a579a63-c226-4f41-8c09-e089ab13e7ea",
   "metadata": {},
   "source": [
    "## Initial exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "20eeaa7e-90fe-451e-93ea-c315b029f174",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['axion1_40-250GeV_100k.h5',\n",
      " 'axion1_40-250GeV_100k_mass0p5GeV.h5',\n",
      " 'axion2_40-250GeV_100k.h5',\n",
      " 'axion2_40-250GeV_100k_mass0p5GeV.h5',\n",
      " 'gamma_40-250GeV_100k.h5',\n",
      " 'gamma_40-250GeV_100k_mass0p5GeV.h5',\n",
      " 'pi0_40-250GeV_100k.h5',\n",
      " 'pi0_40-250GeV_100k_mass0p5GeV.h5',\n",
      " 'scalar1_40-250GeV_100k.h5',\n",
      " 'scalar1_40-250GeV_100k_mass0p5GeV.h5']\n"
     ]
    }
   ],
   "source": [
    "datasets = sorted(os.listdir(f\"{data_dir}/raw_files/h5\"))\n",
    "pprint(datasets)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7689ef49-4451-4ffc-b30c-0aba7b7b03ca",
   "metadata": {},
   "source": [
    "This is a 3-class classification task. These two particles always appear:\n",
    "1. $\\gamma$ (gamma)\n",
    "2. $\\pi^0$ (pi0)\n",
    "\n",
    "Each task includes an additional unique particle, which could be one of the following:\n",
    "\n",
    "3. axion1 ($a \\rightarrow \\gamma\\gamma$)\n",
    "4. axion2 ($a \\rightarrow 3\\pi^0$)\n",
    "5. scalar1 ($s \\rightarrow \\pi^0 \\pi^0$)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e6d17170-4ebc-425c-b1a1-65e053f5257a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== axion1_40-250GeV_100k.h5 ===\n",
      "<class 'h5py._hl.dataset.Dataset'>\n",
      "energy               (100000, 1)\n",
      "layer_0              (100000, 4, 16)\n",
      "layer_1              (100000, 4, 128)\n",
      "layer_2              (100000, 16, 16)\n",
      "layer_3              (100000, 16, 8)\n",
      "overflow             (100000, 4)\n",
      "\n",
      "=== axion1_40-250GeV_100k_mass0p5GeV.h5 ===\n",
      "<class 'h5py._hl.dataset.Dataset'>\n",
      "energy               (100000, 1)\n",
      "layer_0              (100000, 4, 16)\n",
      "layer_1              (100000, 4, 128)\n",
      "layer_2              (100000, 16, 16)\n",
      "layer_3              (100000, 16, 8)\n",
      "overflow             (100000, 4)\n",
      "\n",
      "=== axion2_40-250GeV_100k.h5 ===\n",
      "<class 'h5py._hl.dataset.Dataset'>\n",
      "energy               (100000, 1)\n",
      "layer_0              (100000, 4, 16)\n",
      "layer_1              (100000, 4, 128)\n",
      "layer_2              (100000, 16, 16)\n",
      "layer_3              (100000, 16, 8)\n",
      "overflow             (100000, 4)\n",
      "\n",
      "=== axion2_40-250GeV_100k_mass0p5GeV.h5 ===\n",
      "<class 'h5py._hl.dataset.Dataset'>\n",
      "energy               (100000, 1)\n",
      "layer_0              (100000, 4, 16)\n",
      "layer_1              (100000, 4, 128)\n",
      "layer_2              (100000, 16, 16)\n",
      "layer_3              (100000, 16, 8)\n",
      "overflow             (100000, 4)\n",
      "\n",
      "=== gamma_40-250GeV_100k.h5 ===\n",
      "<class 'h5py._hl.dataset.Dataset'>\n",
      "energy               (100000, 1)\n",
      "layer_0              (100000, 4, 16)\n",
      "layer_1              (100000, 4, 128)\n",
      "layer_2              (100000, 16, 16)\n",
      "layer_3              (100000, 16, 8)\n",
      "overflow             (100000, 4)\n",
      "\n",
      "=== gamma_40-250GeV_100k_mass0p5GeV.h5 ===\n",
      "OSError encountered.\n",
      "\n",
      "=== pi0_40-250GeV_100k.h5 ===\n",
      "<class 'h5py._hl.dataset.Dataset'>\n",
      "energy               (100000, 1)\n",
      "layer_0              (100000, 4, 16)\n",
      "layer_1              (100000, 4, 128)\n",
      "layer_2              (100000, 16, 16)\n",
      "layer_3              (100000, 16, 8)\n",
      "overflow             (100000, 4)\n",
      "\n",
      "=== pi0_40-250GeV_100k_mass0p5GeV.h5 ===\n",
      "<class 'h5py._hl.dataset.Dataset'>\n",
      "energy               (100000, 1)\n",
      "layer_0              (100000, 4, 16)\n",
      "layer_1              (100000, 4, 128)\n",
      "layer_2              (100000, 16, 16)\n",
      "layer_3              (100000, 16, 8)\n",
      "overflow             (100000, 4)\n",
      "\n",
      "=== scalar1_40-250GeV_100k.h5 ===\n",
      "<class 'h5py._hl.dataset.Dataset'>\n",
      "energy               (100000, 1)\n",
      "layer_0              (100000, 4, 16)\n",
      "layer_1              (100000, 4, 128)\n",
      "layer_2              (100000, 16, 16)\n",
      "layer_3              (100000, 16, 8)\n",
      "overflow             (100000, 4)\n",
      "\n",
      "=== scalar1_40-250GeV_100k_mass0p5GeV.h5 ===\n",
      "<class 'h5py._hl.dataset.Dataset'>\n",
      "energy               (100000, 1)\n",
      "layer_0              (100000, 4, 16)\n",
      "layer_1              (100000, 4, 128)\n",
      "layer_2              (100000, 16, 16)\n",
      "layer_3              (100000, 16, 8)\n",
      "overflow             (100000, 4)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Take a peek into these files\n",
    "for dataset in datasets:\n",
    "    print(f\"=== {dataset} ===\")\n",
    "    \n",
    "    try:\n",
    "        f = h5py.File(f\"raw_files/h5/{dataset}\")\n",
    "    except OSError:\n",
    "        print(f\"OSError encountered.\\n\")\n",
    "        continue\n",
    "\n",
    "    print(type(f[\"energy\"]))\n",
    "\n",
    "    for key in f.keys():\n",
    "        print(key.ljust(20), f[key].shape)        \n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "51bb3f06-ca61-4a74-8c30-9b2b92d02cf7",
   "metadata": {},
   "outputs": [],
   "source": [
    "energy = f.get(\"energy\")[()]\n",
    "overflow = f.get(\"overflow\")[()]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "top-tagging",
   "language": "python",
   "name": "top-tagging"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
